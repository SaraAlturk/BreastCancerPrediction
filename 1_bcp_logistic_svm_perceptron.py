# -*- coding: utf-8 -*-
"""1_BCP_Logistic_SVM_Perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BCChl1pKK-VjJMP2_SmQuOiNwnqXCcqd

# Breast Cancer Prediction : Logistic Regression, Perceptron, and Support Vector Machine
--------------------------

# Dataset Preprocessing
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

""" ## 1. Load and view the dataset."""

df = pd.read_csv('/content/sample_data/Breast-Cancer.csv')

# display first 5 columns
display(df.head(5))
# check dimensions
print(df.shape)
# list column names
print(df.columns)
# datatypes
print(df.info())

"""## 2. Preprocess the data by normalizing it and handling missing values outliers, and categorical variables.

### Missing Values &  unnecessary columns
"""

# checking for missing values
print("missing values:\n\n",df.isnull().sum())
# checking for duplicated values
print("\nduplicated values:", df.duplicated().sum())

"""The Unnamed: 32 column has all its values set to null.


"""

# Drop unnecessary columns
# Drop the Unnamed: 32 & id column since they have no significance.
df.drop(['id', 'Unnamed: 32'], axis=1, inplace=True)

print(df.columns)

"""### Outliers

"""

# Function to calculate outliers using IQR method
def count_outliers(data):
    outliers = {}
    for column in data.select_dtypes(include=[np.number]).columns:
        Q1 = data[column].quantile(0.25)
        Q3 = data[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        # Count outliers
        outliers[column] = data[(data[column] < lower_bound) | (data[column] > upper_bound)][column].count()
    return outliers

# Count outliers
outlier_counts = count_outliers(df)
outlier_counts_df = pd.DataFrame(list(outlier_counts.items()), columns=['Feature', 'Outlier Count'])

# Display outlier counts
print(outlier_counts_df)

# Visualizing outliers
# Set up the figure and axes for a grid layout
features = df.select_dtypes(include=[float]).columns  # Select only numerical columns
num_features = len(features)
num_cols = 3  # Number of columns for the plot grid
num_rows = (num_features // num_cols) + 1  # Calculate rows based on total features

fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 20))
fig.suptitle("Distribution of Features by Diagnosis", fontsize=16, y=1.02)

# Flatten axes array for easy iteration
axes = axes.flatten()

# Create boxplots for each feature
for i, feature in enumerate(features):
    sns.boxplot(data=df, x='diagnosis', y=feature, palette=['#FF69B4', '#6495ED'], ax=axes[i])
    axes[i].set_title(f"Distribution of {feature} by Diagnosis")
    axes[i].set_xlabel("Diagnosis")
    axes[i].set_ylabel(feature)

# Hide any empty subplots if features < rows * cols
for j in range(i + 1, num_rows * num_cols):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""### Target Feature Encoding & Distribution

the target feature (diagnosis) is an object that needs encoding


"""

df['diagnosis'] = df['diagnosis'].map({'B': 0, 'M':1})
# count values in each category
df['diagnosis'].value_counts()

import warnings
warnings.filterwarnings('ignore')

custom_colors = ['#FF69B4', '#6495ED']  #  custom colors for pink and blue

# Set up the figure with two subplots
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Bar chart for counts
sns.countplot(x='diagnosis', data=df, palette=custom_colors, ax=axes[0])
for p in axes[0].patches:
    axes[0].annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2, p.get_height() + 5), ha='center')
axes[0].set_title('Diagnosis Counts')
axes[0].set_xlabel('Diagnosis')
axes[0].set_xticklabels(['Benign', 'Malignant'])
axes[0].set_ylabel('Count')

# Pie chart for rate
axes[1].pie(df['diagnosis'].value_counts(),
            labels=['Benign', 'Malignant'],  # Set labels
            autopct='%1.2f%%',
            startangle=90,
            colors=custom_colors)
axes[1].set_title('Diagnosis Rate')
axes[1].axis('equal')  # Equal aspect ratio ensures the pie is drawn as a circle

# Display the plots
plt.show()

"""Target feature labels are imbalanced with more Benign than Melignant

### Correlation Heatmap

Darker colors might indicate stronger correlations (either positive or negative). Lighter colors suggest weaker correlations.
"""

corr_matrix = df.loc[:, df.columns != 'diagnosis'].corr()
plt.figure(figsize=(17, 10))
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Heatmap of Breast Cancer Features', size=15)
plt.show()

corr_vector = df.corr(numeric_only=True)['diagnosis'].sort_values(ascending=False)
corr_vector.plot(kind='barh', figsize=(10, 6), color='#FF69B4')
plt.title('Correlation of diagnosis with the other Features')
plt.ylabel('Features')
plt.xlabel('Correlation')
plt.show()

"""`smoothness_se`, `fractal_dimension_mean`, `texture_se`, `ymmetry_se` and `fractal_dimension_se.` have low correlations ,i will consider dropping them

High multicollinearity between high correlates features : aim to keep a subset that provides the most unique information.
"""

# Set the correlation threshold
threshold = 0.6

# Calculate correlation with the target feature 'diagnosis'
corr_matrix = df.corr(numeric_only=True)
high_corr_features_list = corr_matrix.index[abs(corr_matrix['diagnosis']) > threshold].tolist()

# Remove the target column itself from the list to avoid including it
high_corr_features_list.remove('diagnosis')

# Create a new DataFrame with only the high-correlation features
high_corr_features = df[high_corr_features_list]

# Display the selected high-correlation features
print("High-correlation features (correlation > 0.6):", high_corr_features.columns.tolist())
high_corr_features.head()
# Assuming `high_corr_features` is a DataFrame with the high-correlation features
corr_matrix = high_corr_features.corr()

# Plot the correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Matrix of High-Correlation Features")
plt.show()

"""`radius_mean` and `perimeter_mean` (corr = 1): Keep one : `radius_mean` (since they are the same).

`radius_worst`, `perimeter_worst`, and `area_worst`: are all highly correlated with each other so  keep one : `radius_worst`

`concave points_mean` and `concave points_worst `: drop `concave points_worst`.

`concavity_mean` has a higher correlation with diagnosis, keep it and drop `concavity_worst`

### Dropping More Columns
"""

# drop :smoothness_se , area_mean,  perimeter_mean , perimeter_worst , area_worst,  concave points_worst,  concavity_worst
df.drop(['smoothness_se', 'area_mean','perimeter_mean','perimeter_worst','concave points_worst','concavity_worst'], axis=1, inplace=True)

print(df.columns)

"""##3. Split the dataset into training and testing sets with an 80:20 ratio."""

#  'diagnosis' is the target variable
X = df.drop('diagnosis', axis=1)
y = df['diagnosis']

# Split the data into training and testing sets with an 80:20 ratio, and shuffle
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)

# Display the output
#print("Training target values (y_train):\n", y_train.head(3), "\n")
#print("Training features (X_train):\n", X_train.head(3), "\n")

print("x train and test :\n",X_train.shape,'\n',X_test.shape,"\n\ny train and test :\n",y_train.shape,'\n',y_test.shape)

"""### Data Scaling

"""

# max value = 1 , min value = 0
scaler = StandardScaler()
#Fit on training set only
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# Training Logistic Regression, Perceptron, and Support Vector Machine (SVM)
------

## Logistic Regression

Logistic regression is used for binary classification problems.

It uses the sigmoid function, a mathematical function used to map any real-valued number to a value between 0 and 1.
"""

class LogisticRegression:
    def __init__(self, learning_rate=0.001, n_iterations=10000):
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.W = None
        self.b = None
        self.X = None
        self.y = None
        self.loss_history = []  # Initialize loss_history here

    def sigmoid(self, z):
        #Sigmoid activation function
        return 1 / (1 + np.exp(-z))

    def initialize_parameter(self, X):
        #Initializes weights and bias
        self.W = np.zeros(X.shape[1])
        self.b = 0.0

    def forward(self, X):
        #Computes forward propagation for given input X
        Z = np.matmul(X, self.W) + self.b
        A = self.sigmoid(Z)
        return A

    def compute_cost(self, predictions):
        #Computes the cost function for given predictions
        m = self.X.shape[0]  # number of training examples
        cost = np.sum((-np.log(predictions + 1e-8) * self.y) + (-np.log(1 - predictions + 1e-8)) * (1 - self.y))
        cost = cost / m
        return cost

    def compute_gradient(self, predictions):
        #Computes the gradients for weights and bias
        m = self.X.shape[0]
        dw = (1 / m) * np.dot(self.X.T, (predictions - self.y))
        db = (1 / m) * np.sum(predictions - self.y)
        return dw, db

    def fit(self, X, y):
        #Trains the logistic regression model using gradient descent.
        self.X = X
        self.y = y
        self.initialize_parameter(X)

        for i in range(self.n_iterations):
            # Forward propagation
            predictions = self.forward(X)

            # Compute the cost
            cost = self.compute_cost(predictions)
            self.loss_history.append(cost)

            # Compute gradients
            dw, db = self.compute_gradient(predictions)

            # Update parameters
            self.W -= self.learning_rate * dw
            self.b -= self.learning_rate * db

            # Print cost every 100 iterations
            if i % 10000 == 0:
                print(f"Cost after iteration {i}: {cost}")


    def predict(self, X):
        """Predicts binary labels for input data."""
        predictions = self.forward(X)
        y_pred_class = [1 if i > 0.5 else 0 for i in predictions]
        return np.array(y_pred_class)

# Instantiate the model
model = LogisticRegression(learning_rate=0.0001, n_iterations=100000)

# Train the model
#model.fit(X_train, y_train)
# Train the model with scaled features
model.fit(X_train_scaled, y_train)

# Make predictions
y_pred = model.predict(X_test_scaled)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

from sklearn.metrics import ConfusionMatrixDisplay

# Confusion matrix for Logistic Regression
ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=['Benign', 'Malignant'], cmap='Purples')
plt.title("Confusion Matrix - Logistic Regression")
plt.show()

"""True Positives (TP): predicted 40 instances as "Malignant".


True Negatives (TN): predicted 70 instances as "Benign".

False Positives (FP): incorrectly predicted 1 instance as "Malignant" when it's "Benign".

False Negatives (FN): incorrectly predicted 1 instance as "Benign" when they were actually "Malignant".

## Support Vector Machine (SVM)
"""

class SVM:
    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iterations=1000):
        self.learning_rate = learning_rate
        self.lambda_param = lambda_param
        self.n_iterations = n_iterations
        self.w = None
        self.b = None

    def fit(self, X, y):
        n_samples, n_features = X.shape
        y_ = np.where(y <= 0, -1, 1)  # convert 0 labels to -1 for SVM

        # Initialize weights and bias
        self.w = np.zeros(n_features)
        self.b = 0

        # Gradient Descent
        for _ in range(self.n_iterations):
            for idx, x_i in enumerate(X):
                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1
                if condition:
                    dw = self.lambda_param * self.w
                    db = 0
                else:
                    dw = self.lambda_param * self.w - np.dot(x_i, y_[idx])
                    db = y_[idx]
                self.w -= self.learning_rate * dw
                self.b -= self.learning_rate * db

    def predict(self, X):
        linear_output = np.dot(X, self.w) - self.b
        return np.where(linear_output >= 0, 1, 0)

"""Use grid search to find the best hyperparameters for the SVM model.

this section uses grid search to  iterate over combinations of hyperparameters (learning rate, lambda, and iterations) to find the best parameters based on validation accuracy.
"""

# Define the grid search function
def svm_grid_search(X_train, y_train, X_val, y_val, param_grid):
    best_score = 0
    best_params = {}

    # Iterate over all combinations of parameters in the grid
    for lr in param_grid['learning_rate']:
        for lp in param_grid['lambda_param']:
            for n_iter in param_grid['n_iterations']:
                # Initialize and train the model with current hyperparameters
                svm = SVM(learning_rate=lr, lambda_param=lp, n_iterations=n_iter)
                svm.fit(X_train, y_train)

                # Validate the model
                y_pred = svm.predict(X_val)
                accuracy = accuracy_score(y_val, y_pred)

                # Update best parameters if current accuracy is better
                if accuracy > best_score:
                    best_score = accuracy
                    best_params = {'learning_rate': lr, 'lambda_param': lp, 'n_iterations': n_iter}

                print(f"LearningR: {lr}, LambdaP: {lp}, iteration: {n_iter} - Accuracy: {accuracy}")

    print("\nBest Score:", best_score)
    print("Best Params:", best_params)
    return best_params

# Define parameter grid for grid search
param_grid = {
    'learning_rate': [0.0001, 0.001, 0.01],
    'lambda_param': [0.001, 0.01, 0.1],
    'n_iterations': [1000, 5000, 10000]
}

# Split training data further into a validation set for hyperparameter tuning
X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)

# Perform grid search to find the best parameters
best_params = svm_grid_search(X_train_sub, y_train_sub, X_val, y_val, param_grid)

# Train final model with best parameters
best_svm_model = SVM(learning_rate=best_params['learning_rate'],
                     lambda_param=best_params['lambda_param'],
                     n_iterations=best_params['n_iterations'])
best_svm_model.fit(X_train_scaled, y_train)

# Evaluate final model on test data
y_pred_best = best_svm_model.predict(X_test_scaled)
print("\nFinal Model Accuracy:", accuracy_score(y_test, y_pred_best))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_best))
print("Classification Report:\n", classification_report(y_test, y_pred_best))

# Instantiate and train SVM
svm_model = SVM(learning_rate=0.0001, lambda_param=0.01, n_iterations=1000)
svm_model.fit(X_train_scaled, y_train)

# Predict and evaluate
y_pred_svm = svm_model.predict(X_test_scaled)
print("SVM Accuracy:", accuracy_score(y_test, y_pred_svm))
print("SVM Confusion Matrix:\n", confusion_matrix(y_test, y_pred_svm))
print("SVM Classification Report:\n", classification_report(y_test, y_pred_svm))

# Confusion matrix for SVM
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_best, display_labels=['Benign', 'Malignant'], cmap='Blues')
plt.title("Confusion Matrix - SVM")
plt.show()

"""True Positives (TP): The model correctly predicted 41 instances as "Malignant"

True Negatives (TN): The model correctly predicted 71 instances as "Benign"

False Positives (FP): The model made no incorrect predictions of "Benign" as "Malignant"

False Negatives (FN): The model incorrectly predicted 2 instances as "Benign" when they were actually "Malignant"

## Perceptron
"""

class Perceptron:
    def __init__(self, num_inputs, learning_rate=0.01, n_iterations=1000):
        # Initialize weights with random values, including one extra for bias
        self.weights = np.random.rand(num_inputs + 1)
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations

    # Linear combination of inputs and weights
    def linear(self, inputs):
        return np.dot(inputs, self.weights[1:]) + self.weights[0]

    # Heaviside step function as the activation
    def Heaviside_step_fn(self, z):
        return 1 if z >= 0 else 0

    # Predict method using the linear layer and activation function
    def predict(self, inputs):
        Z = self.linear(inputs)
        # Handles both single sample and batch inputs
        if np.isscalar(Z):
            return self.Heaviside_step_fn(Z)
        return np.array([self.Heaviside_step_fn(z) for z in Z])

    # Compute the error (loss) for a given prediction and target
    def loss(self, prediction, target):
        return target - prediction

    # Train method for updating weights based on error
    def train(self, inputs, target):
        prediction = self.predict(inputs)
        error = self.loss(prediction, target)
        # Weight update based on prediction error
        self.weights[1:] += self.learning_rate * error * inputs
        self.weights[0] += self.learning_rate * error  # Update bias

    # Fit method to train over multiple epochs
    def fit(self, X, y):
        for _ in range(self.n_iterations):
            for inputs, target in zip(X, y):
                self.train(inputs, target)

# Instantiate and train Perceptron
perceptron_model = Perceptron(num_inputs=X_train_scaled.shape[1], learning_rate=0.001, n_iterations=10000)
perceptron_model.fit(X_train_scaled, y_train)

# Predict and evaluate
y_pred_perceptron = perceptron_model.predict(X_test_scaled)
print("Perceptron Accuracy:", accuracy_score(y_test, y_pred_perceptron))
print("Perceptron Confusion Matrix:\n", confusion_matrix(y_test, y_pred_perceptron))
print("Perceptron Classification Report:\n", classification_report(y_test, y_pred_perceptron))

# Confusion matrix for Perceptron
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_perceptron, display_labels=['Benign', 'Malignant'], cmap='Reds')
plt.title("Confusion Matrix - Perceptron")
plt.show()

"""True Positives (TP): The model correctly predicted 42 instances as "Malignant".

True Negatives (TN): The model correctly predicted 65 instances as "Benign".

False Positives (FP): The model incorrectly predicted 6 instances as "Malignant" when they were actually "Benign".

False Negatives (FN): The model incorrectly predicted 2 instance as "Benign" when it was actually "Malignant".
"""